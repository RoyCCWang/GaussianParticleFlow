# state, weight, moment updates from Bunch's paper.

function computeð‘šBunch(x, Î», Ïˆ, m0, inv_P0, inv_R, y)

    H = ForwardDiff.jacobian(Ïˆ, x)
    y_hat = y - Ïˆ(x) + H*x

    inv_P_Î»_x = inv_P0 + Î»*H'*inv_R*H
    P_Î»_x = inv(inv_P_Î»_x)

    ð‘š = P_Î»_x*( inv_P0*m0 + Î»*H'*inv_R*y_hat )

    return ð‘š
end

function computestateupdateBunch(x, Î»0, Î»1, Ïˆ, m0, inv_P0, inv_R, y, Î³ = 0.0)

    m_Î»1 = computeð‘šBunch(x, Î»1, Ïˆ, m0, inv_P0, inv_R, y)
    m_Î»0 = computeð‘šBunch(x, Î»0, Ïˆ, m0, inv_P0, inv_R, y)

    G = computeðºBunch(x, Î»0, Î»1, Ïˆ, inv_P0, inv_R)

    Î²_Î³ = exp(-0.5*Î³*(Î»1-Î»0))

    return m_Î»1 + Î²_Î³ .* (G*(x-m_Î»0))
end



##### covariance-related updates.

# Let P := P_Î»(x). This is the general case as reported by Bunch's eqn 18.
# Compute via eqn 18: P, inv(P), and sqrt(Q), where Q := P*inv(P).
function computePquantitiesBunch(x, Î», Ïˆ, inv_P0, inv_R)

    H = ForwardDiff.jacobian(Ïˆ, x)

    inv_P_Î»_x = inv_P0 + Î»*H'*inv_R*H

    P_Î»_x = inv(inv_P_Î»_x)

    return P_Î»_x, inv_P_Î»_x#, Q, sqrt_Q
end


# define ð‘ƒ1 := sqrt( P_Î»1*inv(P_Î»0) ) at x_Î»0.
# x is x_Î»0 in this function.
# Ïˆ: â„á´° â†’ â„á´¸ here; a vector-valued function, even if L == 1.
function computeðºBunch(x, Î»0, Î»1, Ïˆ, inv_P0, inv_R)

    H = ForwardDiff.jacobian(Ïˆ, x)

    inv_P_Î»1_x = inv_P0 + Î»1*H'*inv_R*H
    P_Î»1_x = inv(inv_P_Î»1_x)

    inv_P_Î»0_x = inv_P0 + Î»0*H'*inv_R*H

    # println(" P_Î»1_x*inv_P_Î»0_x = ",  P_Î»1_x*inv_P_Î»0_x)
    ðº = sqrt( P_Î»1_x*inv_P_Î»0_x )

    # return ðº

    ðº_real = real.(ðº)

    return ðº_real
end

# Use numerical differentiation to compute df, where f: â„á´° â†’ â„^{MxN}.
function computegradientformatrixfunctionND(x::Vector{T}, f, M, N) where T
    D = length(x)

    df = Matrix{Vector{T}}(undef, M, N)

    for j = 1:N
        for i = 1:M

            h = xx->f(xx)[i,j]

            df[i,j] = Calculus.gradient(h, x)
        end
    end

    return df
end
