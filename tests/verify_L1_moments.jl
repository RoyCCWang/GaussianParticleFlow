using Distributed

@everywhere import HCubature
@everywhere import Utilities
@everywhere import Printf
@everywhere import PyPlot
@everywhere import Random

@everywhere using LinearAlgebra
@everywhere import Interpolations

@everywhere using FFTW

@everywhere import Statistics

@everywhere import Distributions

@everywhere import Calculus

@everywhere import ForwardDiff
@everywhere import StatsFuns

using Test
using BenchmarkTools

import VisualizationTools

#@everywhere import Seaborn
@everywhere include("../src/misc/declarations.jl")

@everywhere include("../src/SDE/Brownian.jl")
@everywhere include("../src/flows/approx_flow.jl")
@everywhere include("../tests/routines/sde.jl")
@everywhere include("../src/flows/moments.jl")
@everywhere include("../src/flows/derivatives.jl")
@everywhere include("../src/misc/utilities.jl")
@everywhere include("../src/misc/utilities2.jl")

@everywhere include("../tests/routines/simulation_tools.jl")
@everywhere include("../src/flows/exact_flow.jl")

@everywhere include("../src/importance_sampler/IS_engine.jl")
@everywhere include("../src/diagnostics/functionals.jl")


@everywhere include("../src/flows/traverse_with_weights.jl")
@everywhere include("../src/flows/traverse_no_weights.jl")


### adaptive kernel.
@everywhere include("../fresh/adaptive_kernel/scalar.jl")
@everywhere include("../fresh/adaptive_kernel/traverse.jl")

@everywhere include("../tests/routines/L1_moments.jl")
@everywhere include("../tests/routines/Bunch_updates.jl")

## TODO I am here. go through routines to ensure as fast as possible
#    implementation of GFlow, with options to cache Brownian motion, or
#    draw on demand.

# how can transport help with likelihoods?

PyPlot.close("all")
fig_num = 1

Random.seed!(25)
#Random.seed!(435)
#Random.seed!(244432)


N_batches = 16



#
max_integral_evals = typemax(Int) #1000000
initial_div = 1000



D_x = 6
胃_a = rand()

A_ = randn(D_x, D_x)
 = xx->sinc(dot(xx,A_*xx))
d = 
d2 = 

# hardcode set up for L = 1, for now.
虏 = rand()
虏_vec = [ 虏 ]
R = ones(Float64, 1, 1)
R[1] = 虏

P_0 = diagm( 虏 .* ones(D_x) )
m_0 = randn(D_x) # this is z_input.
z_input = m_0

 = xx->[ (xx) ]
y = randn(1)

prior_dist = Distributions.MvNormal(m_0, P_0)

ln_prior_pdf_func = xx->Distributions.logpdf(prior_dist, xx)
ln_likelihood_func = xx->evallnMVNlikelihood(xx, y, m_0, P_0, , R)



###### verify moment expressions. pakage into test script when done.
L = 1

x0 = randn(D_x)
位0 = rand()
位1 = 位0 + rand()




inv_P0 = diagm(1/虏 * ones(Float64, D_x))
inv_R = diagm(1/虏 * ones(Float64, L))

getPquantities_Bunch = (xx,位位)->computePquantitiesBunch(xx, 位位, , inv_P0, inv_R)

getPquantities_mine = (xx,位位)->computePquantitiesL1(xx, 位位, , 虏)

B, inv_B = getPquantities_Bunch(x0, 位0)
C, inv_C = getPquantities_mine(x0, 位0)

println("verify my P update.")
println("l-2 discrepancy of P: mine vs. Bunch is ", norm(B-C))
println("l-2 discrepancy of inv(P): mine vs. Bunch is ", norm(inv_B-inv_C))
println()



# verify sqrtm formula.
u = randn(D_x)
K = rand()* 5.3

Q = LinearAlgebra.I + K .* u*u'

x = ( sqrt(1+dot(u,u)*K)-1 )/dot(u,u)
sqrt_Q = LinearAlgebra.I + x .* u*u'
println("verify sqrt formula.")
println("l-2 discrepancy of sqrt(Q) vs. Q is ", norm(sqrt_Q-sqrt(Q)))
println()



### verify formula for d(u岬u).

function evaldu岬u(, x::Vector{T})::Vector{T} where T
    D = length(x)

    d_x = ForwardDiff.gradient(, x)
    d2_x = ForwardDiff.hessian(, x)

    return collect( sum( 2*d_x[i]*d2_x[i,j] for i = 1:D ) for j = 1:D )
end

function evalu岬u(, x::Vector{T})::T where T

    u = ForwardDiff.gradient(, x)

    return dot(u,u)
end

_scalar = xx->(xx)[1]

f = xx->evalu岬u(_scalar, xx)
df_AD = xx->ForwardDiff.gradient(f, xx)

df_AN = xx->evaldu岬u(_scalar, xx)

## comment out for speed.
A = df_AD(x0)
B = df_AN(x0)

println("verify d(u岬u) formula.")
println("d(u岬u), L = 1: l-2 discrepancy between AN and AD is ", norm(A-B))
println()



# skip to speed up.
### verify d formula.
get_Bunch = xx->computeBunch(xx, 位0, 位1, , inv_P0, inv_R)
getquantities_mine = xx->compute(xx, 位0, 位1, _scalar, 虏)

G_Bunch = get_Bunch(x0)
dG_Bunch = computegradientformatrixfunctionND(x0, get_Bunch, D_x, D_x)

G_mine, dG_mine = getquantities_mine(x0)

println("verify  formula.")
println(", L = 1: l-2 discrepancy between my formula and Bunch is  ", norm(G_Bunch-G_mine))
println("d, L = 1: l-2 discrepancy between my formula and Bunch is ", norm(dG_Bunch-dG_mine))
println()


### Verify d(y_hat).

# intermediate tests, leading up to d.
y_hat_func = xx->computeyhatL1(xx, , y)
dy_hat_AD = xx->ForwardDiff.jacobian(y_hat_func, xx)

d2_scalar = xx->ForwardDiff.hessian(_scalar, xx)

dy_hat_x0_mine = collect( dot( d2_scalar(x0)[:,j], x0) for j = 1:D_x )
dy_hat_x0_AD = vec(dy_hat_AD(x0))

println("dy_hat discrepancy: mine vs. AD is ", norm(dy_hat_x0_mine-dy_hat_x0_AD))
println()

# verify 魏.
y_scalar = y[1]
魏_func = xx->compute魏(xx, z_input, 位0, _scalar, 虏, y_scalar)
魏_x0 = 魏_func(x0)

d魏_AD = xx->ForwardDiff.gradient(魏_func, xx)
d魏_AD_x0 = d魏_AD(x0)

d魏 = xx->computed魏(xx, z_input, 位0, _scalar, 虏, y_scalar)
d魏_x0 = d魏(x0)

println("d魏 discrepancy: mine vs. AD is ", norm(d魏_x0-d魏_AD_x0))
println()




##### verify m_位(x), and its Jacobian.
get_Bunch = xx->computeBunch(xx, 位0, , z_input, inv_P0, inv_R, y)

getd_Bunch = xx->computedBunch(xx, 位0, , z_input, inv_P0, inv_R, y)

_Bunch = get_Bunch(x0)

d_Bunch_mat = ForwardDiff.jacobian(xx->get_Bunch(xx), x0)
d_Bunch = collect( vec(d_Bunch_mat[j,:]) for j = 1:D_x )

#_mine, d_mine, dk_mine = getquantities_mine(x0)

_func_mine = xx->compute(xx, z_input, 位0, _scalar, 虏, y_scalar)
d_AD = xx->ForwardDiff.jacobian(_func_mine, xx)

d_mine = xx->computed(xx, z_input, 位0, _scalar, 虏, y_scalar)

d_mine_x0 = d_mine(x0)
d_mine_x0_mat = convertnestedvector(d_mine_x0)
#d_mine_x0_mat = d_mine_x0_mat'

d_AD_x0 = d_AD(x0)

println("d: l-2 discrepancy between AN and AD is ", norm(d_AD_x0-d_mine_x0_mat))




### verify state update and its Jacobian.

x1_func_Bunch = xx->computestateupdateBunch(xx, 位0, 位1, , z_input, inv_P0, inv_R, y, 0.0)
x1_func = xx->computestateupdate(xx, 位0, 位1, _scalar,
                                z_input, 虏, y_scalar)

x1_Bunch = x1_func_Bunch(x0)
x1 = x1_func(x0)

dx1_func_AD = xx->ForwardDiff.jacobian(x1_func, xx)
dx1_func = xx->computedx1L1(xx, 位0, 位1, _scalar, z_input, 虏, y_scalar)

dx1_x0_AD = dx1_func_AD(x0)
dx1_x0 = dx1_func(x0)

println("verify  formula.")
println("x1, L = 1: l-2 discrepancy between my formula and Bunch is  ", norm(x1_Bunch-x1))
println("dx1, L = 1: l-2 discrepancy between my formula and Bunch is  ", norm(dx1_x0_AD-dx1_x0))
println()
